#!/bin/bash

# reset the state of full_plans and checkpoints
rm -rf /root/shared/checkpoints
file="/root/codes/MetisOnAlpa/full_plans.txt"

> "$file" 
echo "0" >> "$file"


model="$1"
GBS="$2"
MAX_PROFILED_TP="$3"
MAX_PROFILED_BATCH_SIZE="$4"
SCALE_VARIANCE="$5"
MAX_PERMUTE_LEN="$6"
NITER="$7"
FULL="$8"

if [[ -z "$model" || -z "$GBS" || -z "$MAX_PROFILED_TP" || -z "$MAX_PROFILED_BATCH_SIZE" || -z "$SCALE_VARIANCE" || -z "$MAX_PERMUTE_LEN" || -z "$NITER" || -z "$FULL" ]]; then
  echo "Usage: $0 MODEL GBS MAX_PROFILED_TP MAX_PROFILED_BATCH_SIZE SCALE_VARIANCE MAX_PERMUTE_LEN NITER FULL"
  exit 1
fi


if [[ "$model" == GPT-* ]]; then
  MODEL_NAME="GPT"
  model_size="${model#GPT-}"
  case "$model_size" in
    "125M") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=768 NUM_LAYERS=12 ATTENTION_HEAD_SIZE=12 VOCAB_SIZE=51200 ;;
    "350M") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=1024 NUM_LAYERS=24 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=51200 ;;
    "760M") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=1536 NUM_LAYERS=24 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=51200 ;;
    "1.3B") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=2048 NUM_LAYERS=24 ATTENTION_HEAD_SIZE=32 VOCAB_SIZE=51200 ;;
    "2.6B") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=2560 NUM_LAYERS=32 ATTENTION_HEAD_SIZE=32 VOCAB_SIZE=51200 ;;
    "6.7B") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=4096 NUM_LAYERS=32 ATTENTION_HEAD_SIZE=32 VOCAB_SIZE=51200 ;;
    "15B")  SEQUENCE_LENGTH=1024 HIDDEN_SIZE=5120 NUM_LAYERS=48 ATTENTION_HEAD_SIZE=40 VOCAB_SIZE=51200 ;;
    "39B")  SEQUENCE_LENGTH=1024 HIDDEN_SIZE=8192 NUM_LAYERS=48 ATTENTION_HEAD_SIZE=64 VOCAB_SIZE=51200 ;;
    "76B")  SEQUENCE_LENGTH=1024 HIDDEN_SIZE=10240 NUM_LAYERS=60 ATTENTION_HEAD_SIZE=80 VOCAB_SIZE=51200 ;;
    *) echo "Unknown GPT model size: $model_size"; exit 1 ;;
  esac

elif [[ "$model" == MoE-* ]]; then
  MODEL_NAME="MoE"
  model_size="${model#MoE-}"
  case "$model_size" in
    "380M") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=768 NUM_LAYERS=8 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    "690M") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=768 NUM_LAYERS=8 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    "1.3B") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=768 NUM_LAYERS=16 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    "2.4B") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=1024 NUM_LAYERS=16 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    "7.1B") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=1280 NUM_LAYERS=16 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    "10B")  SEQUENCE_LENGTH=1024 HIDDEN_SIZE=1536 NUM_LAYERS=16 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    "27B")  SEQUENCE_LENGTH=1024 HIDDEN_SIZE=2048 NUM_LAYERS=16 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    "70B")  SEQUENCE_LENGTH=1024 HIDDEN_SIZE=2048 NUM_LAYERS=32 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    "140B") SEQUENCE_LENGTH=1024 HIDDEN_SIZE=2048 NUM_LAYERS=32 ATTENTION_HEAD_SIZE=16 VOCAB_SIZE=32000 ;;
    *) echo "Unknown MoE model size: $model_size"; exit 1 ;;
  esac

else
  echo "Invalid model type: $model. Expected format: GPT-XXX or MoE-XXX"
  exit 1
fi

HOME_DIR="/root/Metis"


# 调用 cost_het_cluster.sh
source ~/Metis/scripts/cost_het_cluster.sh \
  MODEL_NAME=$MODEL_NAME \
  MODEL_SIZE=$model_size \
  NUM_LAYERS=$NUM_LAYERS \
  GBS=$GBS \
  HIDDEN_SIZE=$HIDDEN_SIZE \
  SEQUENCE_LENGTH=$SEQUENCE_LENGTH \
  VOCAB_SIZE=$VOCAB_SIZE \
  ATTENTION_HEAD_SIZE=$ATTENTION_HEAD_SIZE \
  HOME_DIR="$HOME_DIR" \
  MAX_PROFILED_TP=$MAX_PROFILED_TP \
  MAX_PROFILED_BATCH_SIZE=$MAX_PROFILED_BATCH_SIZE \
  SCALE_VARIANCE=$SCALE_VARIANCE \
  MAX_PERMUTE_LEN=$MAX_PERMUTE_LEN  \
  FULL=$FULL

HOSTFILE="/root/Metis/hostfile"
if [[ ! -f $HOSTFILE ]]; then
    echo "hostfile 不存在: $HOSTFILE"
    exit 1
fi

NUM_HOSTS=$(wc -l < "$HOSTFILE")
NUM_DEVICES_PER_HOST=$(awk '{print $2}' "$HOSTFILE" | head -n 1)


if [[ "$MODEL_NAME" == "GPT" ]]; then
    SUITE_NAME="gpt.perf_test_metis"
else
    SUITE_NAME="moe.perf_test_metis"
fi


python3 ~/alpa/benchmark/alpa/benchmark.py \
  --suite "$SUITE_NAME" \
  --num-hosts "$NUM_HOSTS" \
  --num-devices-per-host "$NUM_DEVICES_PER_HOST" \
  --niter "$NITER"


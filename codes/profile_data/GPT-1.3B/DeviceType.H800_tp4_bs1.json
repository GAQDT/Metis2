{
  "model": {
    "model_name": "GPT-1.3B",
    "num_layers": 24,
    "parameters": {
      "total_parameters_bytes": 5691244544,
      "parameters_per_layer_bytes": [
        436207616,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        201482240,
        419463168
      ]
    }
  },
  "execution_time": {
    "total_time_ms": 53.63585387743711,
    "forward_backward_time_ms": 28.12969313141684,
    "batch_generator_time_ms": 0.41747093200683594,
    "layernorm_grads_all_reduce_time_ms": 6.883568442780938,
    "embedding_grads_all_reduce_time_ms": 0.02234240476865636,
    "optimizer_time_ms": 16.046161618592713,
    "layer_compute_total_ms": [
      0.4691029109017307,
      1.068959848540352,
      0.9850800274900148,
      0.9769255181018944,
      0.9769255181018944,
      0.9775628042807148,
      0.9822775878719443,
      0.9719534300578127,
      0.9755233598272163,
      0.9804938504767505,
      0.9808761039578339,
      0.9829145677757561,
      0.9873724097043375,
      0.9788372670646599,
      0.9876270898296293,
      0.9806212697613534,
      0.9745034971293055,
      0.9799841587590818,
      0.9857168474922875,
      0.9838062785384755,
      1.0508650359964622,
      0.9806212697613534,
      0.9711882954976399,
      0.9750134401922279,
      0.9908101026148208,
      0.3050402332456146
    ]
  },
  "execution_memory": {
    "total_memory_mb": 3041.195387837308,
    "layer_memory_total_mb": [
      20.18337433092676,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      141.29333282891415,
      58.200414454242534
    ]
  }
}